diff --git a/sklearn/svm/setup.py b/sklearn/svm/setup.py
index 711e868..ea042af 100644
--- a/sklearn/svm/setup.py
+++ b/sklearn/svm/setup.py
@@ -21,7 +21,8 @@ def configuration(parent_package='', top_path=None):
                                 join('src', 'libsvm', 'svm.h')],
                        # Force C++ linking in case gcc is picked up instead
                        # of g++ under windows with some versions of MinGW
-                       extra_link_args=['-lstdc++'],
+                       extra_link_args=['-lstdc++', '-lgomp', '-lrt'],
+                       extra_compile_args=['-O3', '-fopenmp'],
                        )
 
     libsvm_sources = ['libsvm.c']
@@ -36,8 +37,10 @@ def configuration(parent_package='', top_path=None):
                                        join('src', 'libsvm')],
                          libraries=['libsvm-skl'],
                          depends=libsvm_depends,
+                         extra_link_args=['-lstdc++', '-lgomp', '-lrt'],
+                         extra_compile_args=['-O3', '-fopenmp'],
                          )
-
+
     ### liblinear module
     cblas_libs, blas_info = get_blas_info()
     if os.name == 'posix':
@@ -55,9 +58,9 @@ def configuration(parent_package='', top_path=None):
                          include_dirs=[join('..', 'src', 'cblas'),
                                        numpy.get_include(),
                                        blas_info.pop('include_dirs', [])],
-                         extra_compile_args=blas_info.pop('extra_compile_args',
-                                                          []),
+                         extra_compile_args=blas_info.pop('extra_compile_args', []) + ['-O3', '-fopenmp'] ,
                          depends=liblinear_depends,
+                         extra_link_args=['-lstdc++', '-lgomp', '-lrt'],
                          # extra_compile_args=['-O0 -fno-inline'],
                          ** blas_info)
 
@@ -69,6 +72,8 @@ def configuration(parent_package='', top_path=None):
                          sources=libsvm_sparse_sources,
                          include_dirs=[numpy.get_include(),
                                        join("src", "libsvm")],
+                         extra_link_args=['-lstdc++', '-lgomp', '-lrt'],
+                         extra_compile_args=['-O3','-fopenmp'],
                          depends=[join("src", "libsvm", "svm.h"),
                                   join("src", "libsvm",
                                        "libsvm_sparse_helper.c")])
diff --git a/sklearn/svm/src/liblinear/liblinear_helper.c b/sklearn/svm/src/liblinear/liblinear_helper.c
index 06902ae..f6a8153 100644
--- a/sklearn/svm/src/liblinear/liblinear_helper.c
+++ b/sklearn/svm/src/liblinear/liblinear_helper.c
@@ -1,6 +1,7 @@
 #include <stdlib.h>
 #include <numpy/arrayobject.h>
 #include "linear.h"
+#include <omp.h>
 
 /*
  * Convert matrix to sparse representation suitable for libsvm. x is
@@ -197,6 +198,8 @@ struct parameter *set_parameter(int solver_type, double eps, double C,
     param->weight_label = (int *) weight_label;
     param->weight = (double *) weight;
     param->max_iter = max_iter;
+    param->nr_thread = omp_get_max_threads();
+    param->init_sol = NULL;
     return param;
 }
 
diff --git a/sklearn/svm/src/liblinear/linear.cpp b/sklearn/svm/src/liblinear/linear.cpp
index 5ba05c7..daf41b5 100644
--- a/sklearn/svm/src/liblinear/linear.cpp
+++ b/sklearn/svm/src/liblinear/linear.cpp
@@ -1,26 +1,3 @@
-/* 
-   Modified 2011:
-
-   - Make labels sorted in group_classes, Dan Yamins.
-   
-   Modified 2012:
-
-   - Changes roles of +1 and -1 to match scikit API, Andreas Mueller
-        See issue 546: https://github.com/scikit-learn/scikit-learn/pull/546
-   - Also changed roles for pairwise class weights, Andreas Mueller
-        See issue 1491: https://github.com/scikit-learn/scikit-learn/pull/1491
-
-   Modified 2014:
-
-   - Remove the hard-coded value of max_iter (1000), that allows max_iter
-     to be passed as a parameter from the classes LogisticRegression and
-     LinearSVC, Manoj Kumar
-   - Added function get_n_iter that exposes the number of iterations.
-        See issue 3499: https://github.com/scikit-learn/scikit-learn/issues/3499
-        See pull 3501: https://github.com/scikit-learn/scikit-learn/pull/3501
-   
- */
-
 #include <math.h>
 #include <stdio.h>
 #include <stdlib.h>
@@ -29,6 +6,9 @@
 #include <locale.h>
 #include "linear.h"
 #include "tron.h"
+#include <omp.h>
+#include <time.h>
+
 typedef signed char schar;
 template <class T> static inline void swap(T& x, T& y) { T t=x; x=y; y=t; }
 #ifndef min
@@ -50,6 +30,7 @@ static void print_string_stdout(const char *s)
 	fputs(s,stdout);
 	fflush(stdout);
 }
+static void print_null(const char *s) {}
 
 static void (*liblinear_print_string) (const char *) = &print_string_stdout;
 
@@ -67,6 +48,109 @@ static void info(const char *fmt,...)
 static void info(const char *fmt,...) {}
 #endif
 
+static inline int rand_int(const int max)
+{
+	static int seed = (int)clock()+omp_get_thread_num();
+#ifdef CV_OMP
+#pragma omp threadprivate(seed)
+#endif
+	seed = ((seed * 1103515245) + 12345) & 0x7fffffff;
+	return seed%max;
+}
+
+class sparse_operator
+{
+public:
+	static double nrm2_sq(const feature_node *x)
+	{
+		double ret = 0;
+		while(x->index != -1)
+		{
+			ret += x->value*x->value;
+			x++;
+		}
+		return (ret);
+	}
+
+	static double dot(const double *s, const feature_node *x)
+	{
+		double ret = 0;
+		while(x->index != -1)
+		{
+			ret += s[x->index-1]*x->value;
+			x++;
+		}
+		return (ret);
+	}
+
+	static void axpy(const double a, const feature_node *x, double *y)
+	{
+		while(x->index != -1)
+		{
+			y[x->index-1] += a*x->value;
+			x++;
+		}
+	}
+};
+
+class Reduce_Vectors
+{
+public:
+	Reduce_Vectors(int size);
+	~Reduce_Vectors();
+
+	void init(void);
+	void sum_scale_x(double scalar, feature_node *x);
+	void reduce_sum(double* v);
+
+private:
+	int nr_thread;
+	int size;
+	double **tmp_array;
+};
+
+Reduce_Vectors::Reduce_Vectors(int size)
+{
+	nr_thread = omp_get_max_threads();
+	this->size = size;
+	tmp_array = new double*[nr_thread];
+	for(int i = 0; i < nr_thread; i++)
+		tmp_array[i] = new double[size];
+}
+
+Reduce_Vectors::~Reduce_Vectors(void)
+{
+	for(int i = 0; i < nr_thread; i++)
+		delete[] tmp_array[i];
+	delete[] tmp_array;
+}
+
+void Reduce_Vectors::init(void)
+{
+#pragma omp parallel for schedule(static)
+	for(int i = 0; i < size; i++)
+		for(int j = 0; j < nr_thread; j++)
+			tmp_array[j][i] = 0.0;
+}
+
+void Reduce_Vectors::sum_scale_x(double scalar, feature_node *x)
+{
+	int thread_id = omp_get_thread_num();
+
+	sparse_operator::axpy(scalar, x, tmp_array[thread_id]);
+}
+
+void Reduce_Vectors::reduce_sum(double* v)
+{
+#pragma omp parallel for schedule(static)
+	for(int i = 0; i < size; i++)
+	{
+		v[i] = 0;
+		for(int j = 0; j < nr_thread; j++)
+			v[i] += tmp_array[j][i];
+	}
+}
+
 class l2r_lr_fun: public function
 {
 public:
@@ -86,6 +170,8 @@ private:
 	double *C;
 	double *z;
 	double *D;
+	Reduce_Vectors *reduce_vectors;
+
 	const problem *prob;
 };
 
@@ -97,6 +183,9 @@ l2r_lr_fun::l2r_lr_fun(const problem *prob, double *C)
 
 	z = new double[l];
 	D = new double[l];
+
+	reduce_vectors = new Reduce_Vectors(get_nr_variable());
+
 	this->C = C;
 }
 
@@ -104,6 +193,7 @@ l2r_lr_fun::~l2r_lr_fun()
 {
 	delete[] z;
 	delete[] D;
+	delete reduce_vectors;
 }
 
 
@@ -162,12 +252,22 @@ void l2r_lr_fun::Hv(double *s, double *Hs)
 	int l=prob->l;
 	int w_size=get_nr_variable();
 	double *wa = new double[l];
+	feature_node **x=prob->x;
 
-	Xv(s, wa);
+	reduce_vectors->init();
+
+#pragma omp parallel for private(i) schedule(guided)
 	for(i=0;i<l;i++)
+	{
+		feature_node * const xi=x[i];
+		wa[i] = sparse_operator::dot(s, xi);
+
 		wa[i] = C[i]*D[i]*wa[i];
 
-	XTv(wa, Hs);
+		reduce_vectors->sum_scale_x(wa[i], xi);
+	}
+
+	reduce_vectors->reduce_sum(Hs);
 	for(i=0;i<w_size;i++)
 		Hs[i] = s[i] + Hs[i];
 	delete[] wa;
@@ -179,36 +279,24 @@ void l2r_lr_fun::Xv(double *v, double *Xv)
 	int l=prob->l;
 	feature_node **x=prob->x;
 
+#pragma omp parallel for private (i) schedule(guided)
 	for(i=0;i<l;i++)
-	{
-		feature_node *s=x[i];
-		Xv[i]=0;
-		while(s->index!=-1)
-		{
-			Xv[i]+=v[s->index-1]*s->value;
-			s++;
-		}
-	}
+		Xv[i]=sparse_operator::dot(v, x[i]);
 }
 
 void l2r_lr_fun::XTv(double *v, double *XTv)
 {
 	int i;
 	int l=prob->l;
-	int w_size=get_nr_variable();
 	feature_node **x=prob->x;
 
-	for(i=0;i<w_size;i++)
-		XTv[i]=0;
+	reduce_vectors->init();
+
+#pragma omp parallel for private(i) schedule(guided)
 	for(i=0;i<l;i++)
-	{
-		feature_node *s=x[i];
-		while(s->index!=-1)
-		{
-			XTv[s->index-1]+=v[i]*s->value;
-			s++;
-		}
-	}
+		reduce_vectors->sum_scale_x(v[i], x[i]);
+
+	reduce_vectors->reduce_sum(XTv);
 }
 
 class l2r_l2_svc_fun: public function
@@ -225,12 +313,13 @@ public:
 
 protected:
 	void Xv(double *v, double *Xv);
-	void subXv(double *v, double *Xv);
 	void subXTv(double *v, double *XTv);
 
 	double *C;
 	double *z;
 	double *D;
+	Reduce_Vectors *reduce_vectors;
+
 	int *I;
 	int sizeI;
 	const problem *prob;
@@ -244,6 +333,9 @@ l2r_l2_svc_fun::l2r_l2_svc_fun(const problem *prob, double *C)
 
 	z = new double[l];
 	D = new double[l];
+
+	reduce_vectors = new Reduce_Vectors(get_nr_variable());
+
 	I = new int[l];
 	this->C = C;
 }
@@ -253,6 +345,7 @@ l2r_l2_svc_fun::~l2r_l2_svc_fun()
 	delete[] z;
 	delete[] D;
 	delete[] I;
+	delete reduce_vectors;
 }
 
 double l2r_l2_svc_fun::fun(double *w)
@@ -310,12 +403,22 @@ void l2r_l2_svc_fun::Hv(double *s, double *Hs)
 	int i;
 	int w_size=get_nr_variable();
 	double *wa = new double[sizeI];
+	feature_node **x=prob->x;
 
-	subXv(s, wa);
+	reduce_vectors->init();
+
+#pragma omp parallel for private(i) schedule(guided)
 	for(i=0;i<sizeI;i++)
+	{
+		feature_node * const xi=x[I[i]];
+		wa[i] = sparse_operator::dot(s, xi);
+
 		wa[i] = C[I[i]]*wa[i];
 
-	subXTv(wa, Hs);
+		reduce_vectors->sum_scale_x(wa[i], xi);
+	}
+
+	reduce_vectors->reduce_sum(Hs);
 	for(i=0;i<w_size;i++)
 		Hs[i] = s[i] + 2*Hs[i];
 	delete[] wa;
@@ -327,52 +430,23 @@ void l2r_l2_svc_fun::Xv(double *v, double *Xv)
 	int l=prob->l;
 	feature_node **x=prob->x;
 
+#pragma omp parallel for private(i) schedule(guided)
 	for(i=0;i<l;i++)
-	{
-		feature_node *s=x[i];
-		Xv[i]=0;
-		while(s->index!=-1)
-		{
-			Xv[i]+=v[s->index-1]*s->value;
-			s++;
-		}
-	}
-}
-
-void l2r_l2_svc_fun::subXv(double *v, double *Xv)
-{
-	int i;
-	feature_node **x=prob->x;
-
-	for(i=0;i<sizeI;i++)
-	{
-		feature_node *s=x[I[i]];
-		Xv[i]=0;
-		while(s->index!=-1)
-		{
-			Xv[i]+=v[s->index-1]*s->value;
-			s++;
-		}
-	}
+		Xv[i]=sparse_operator::dot(v, x[i]);
 }
 
 void l2r_l2_svc_fun::subXTv(double *v, double *XTv)
 {
 	int i;
-	int w_size=get_nr_variable();
 	feature_node **x=prob->x;
 
-	for(i=0;i<w_size;i++)
-		XTv[i]=0;
+	reduce_vectors->init();
+
+#pragma omp parallel for private(i) schedule(guided)
 	for(i=0;i<sizeI;i++)
-	{
-		feature_node *s=x[I[i]];
-		while(s->index!=-1)
-		{
-			XTv[s->index-1]+=v[i]*s->value;
-			s++;
-		}
-	}
+		reduce_vectors->sum_scale_x(v[i], x[I[i]]);
+
+	reduce_vectors->reduce_sum(XTv);
 }
 
 class l2r_l2_svr_fun: public l2r_l2_svc_fun
@@ -609,7 +683,7 @@ int Solver_MCSVM_CS::Solve(double *w)
 		double stopping = -INF;
 		for(i=0;i<active_size;i++)
 		{
-			int j = i+rand()%(active_size-i);
+			int j = i+rand_int(active_size-i);
 			swap(index[i], index[j]);
 		}
 		for(s=0;s<active_size;s++)
@@ -808,6 +882,7 @@ static int solve_l2r_l1l2_svc(
 	int i, s, iter = 0;
 	double C, d, G;
 	double *QD = new double[l];
+	//int max_iter = 1000;
 	int *index = new int[l];
 	double *alpha = new double[l];
 	schar *y = new schar[l];
@@ -853,14 +928,10 @@ static int solve_l2r_l1l2_svc(
 	{
 		QD[i] = diag[GETI(i)];
 
-		feature_node *xi = prob->x[i];
-		while (xi->index != -1)
-		{
-			double val = xi->value;
-			QD[i] += val*val;
-			w[xi->index-1] += y[i]*alpha[i]*val;
-			xi++;
-		}
+		feature_node * const xi = prob->x[i];
+		QD[i] += sparse_operator::nrm2_sq(xi);
+		sparse_operator::axpy(y[i]*alpha[i], xi, w);
+
 		index[i] = i;
 	}
 
@@ -871,23 +942,17 @@ static int solve_l2r_l1l2_svc(
 
 		for (i=0; i<active_size; i++)
 		{
-			int j = i+rand()%(active_size-i);
+			int j = i+rand_int(active_size-i);
 			swap(index[i], index[j]);
 		}
 
 		for (s=0; s<active_size; s++)
 		{
 			i = index[s];
-			G = 0;
-			schar yi = y[i];
+			const schar yi = y[i];
+			feature_node * const xi = prob->x[i];
 
-			feature_node *xi = prob->x[i];
-			while(xi->index!= -1)
-			{
-				G += w[xi->index-1]*(xi->value);
-				xi++;
-			}
-			G = G*yi-1;
+			G = yi*sparse_operator::dot(w, xi)-1;
 
 			C = upper_bound[GETI(i)];
 			G += alpha[i]*diag[GETI(i)];
@@ -928,12 +993,7 @@ static int solve_l2r_l1l2_svc(
 				double alpha_old = alpha[i];
 				alpha[i] = min(max(alpha[i] - G/QD[i], 0.0), C);
 				d = (alpha[i] - alpha_old)*yi;
-				xi = prob->x[i];
-				while (xi->index != -1)
-				{
-					w[xi->index-1] += d*xi->value;
-					xi++;
-				}
+				sparse_operator::axpy(d, xi, w);
 			}
 		}
 
@@ -1027,6 +1087,7 @@ static int solve_l2r_l1l2_svr(
 	int w_size = prob->n;
 	double eps = param->eps;
 	int i, s, iter = 0;
+	//int max_iter = 1000;
 	int active_size = l;
 	int *index = new int[l];
 
@@ -1058,15 +1119,9 @@ static int solve_l2r_l1l2_svr(
 		w[i] = 0;
 	for(i=0; i<l; i++)
 	{
-		QD[i] = 0;
-		feature_node *xi = prob->x[i];
-		while(xi->index != -1)
-		{
-			double val = xi->value;
-			QD[i] += val*val;
-			w[xi->index-1] += beta[i]*val;
-			xi++;
-		}
+		feature_node * const xi = prob->x[i];
+		QD[i] = sparse_operator::nrm2_sq(xi);
+		sparse_operator::axpy(beta[i], xi, w);
 
 		index[i] = i;
 	}
@@ -1079,7 +1134,7 @@ static int solve_l2r_l1l2_svr(
 
 		for(i=0; i<active_size; i++)
 		{
-			int j = i+rand()%(active_size-i);
+			int j = i+rand_int(active_size-i);
 			swap(index[i], index[j]);
 		}
 
@@ -1089,14 +1144,8 @@ static int solve_l2r_l1l2_svr(
 			G = -y[i] + lambda[GETI(i)]*beta[i];
 			H = QD[i] + lambda[GETI(i)];
 
-			feature_node *xi = prob->x[i];
-			while(xi->index != -1)
-			{
-				int ind = xi->index-1;
-				double val = xi->value;
-				G += val*w[ind];
-				xi++;
-			}
+			feature_node * const xi = prob->x[i];
+			G += sparse_operator::dot(w, xi);
 
 			double Gp = G+p;
 			double Gn = G-p;
@@ -1163,14 +1212,7 @@ static int solve_l2r_l1l2_svr(
 			d = beta[i]-beta_old;
 
 			if(d != 0)
-			{
-				xi = prob->x[i];
-				while(xi->index != -1)
-				{
-					w[xi->index-1] += d*xi->value;
-					xi++;
-				}
-			}
+				sparse_operator::axpy(d, xi, w);
 		}
 
 		if(iter == 0)
@@ -1244,14 +1286,14 @@ static int solve_l2r_l1l2_svr(
 #define GETI(i) (y[i]+1)
 // To support weights for instances, use GETI(i) (i)
 
-int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, double Cn,
-					   int max_iter)
+int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, double Cn, int max_iter)
 {
 	int l = prob->l;
 	int w_size = prob->n;
 	int i, s, iter = 0;
 	double *xTx = new double[l];
-	int *index = new int[l];
+	//int max_iter = 1000;
+	int *index = new int[l];
 	double *alpha = new double[2*l]; // store alpha and C - alpha
 	schar *y = new schar[l];
 	int max_inner_iter = 100; // for inner Newton
@@ -1270,7 +1312,7 @@ int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, dou
 			y[i] = -1;
 		}
 	}
-
+
 	// Initial alpha can be set here. Note that
 	// 0 < alpha[i] < upper_bound[GETI(i)]
 	// alpha[2*i] + alpha[2*i+1] = upper_bound[GETI(i)]
@@ -1284,15 +1326,9 @@ int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, dou
 		w[i] = 0;
 	for(i=0; i<l; i++)
 	{
-		xTx[i] = 0;
-		feature_node *xi = prob->x[i];
-		while (xi->index != -1)
-		{
-			double val = xi->value;
-			xTx[i] += val*val;
-			w[xi->index-1] += y[i]*alpha[2*i]*val;
-			xi++;
-		}
+		feature_node * const xi = prob->x[i];
+		xTx[i] = sparse_operator::nrm2_sq(xi);
+		sparse_operator::axpy(y[i]*alpha[2*i], xi, w);
 		index[i] = i;
 	}
 
@@ -1300,7 +1336,7 @@ int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, dou
 	{
 		for (i=0; i<l; i++)
 		{
-			int j = i+rand()%(l-i);
+			int j = i+rand_int(l-i);
 			swap(index[i], index[j]);
 		}
 		int newton_iter = 0;
@@ -1308,16 +1344,11 @@ int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, dou
 		for (s=0; s<l; s++)
 		{
 			i = index[s];
-			schar yi = y[i];
+			const schar yi = y[i];
 			double C = upper_bound[GETI(i)];
 			double ywTx = 0, xisq = xTx[i];
-			feature_node *xi = prob->x[i];
-			while (xi->index != -1)
-			{
-				ywTx += w[xi->index-1]*xi->value;
-				xi++;
-			}
-			ywTx *= y[i];
+			feature_node * const xi = prob->x[i];
+			ywTx = yi*sparse_operator::dot(w, xi);
 			double a = xisq, b = ywTx;
 
 			// Decide to minimize g_1(z) or g_2(z)
@@ -1359,12 +1390,7 @@ int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, dou
 			{
 				alpha[ind1] = z;
 				alpha[ind2] = C-z;
-				xi = prob->x[i];
-				while (xi->index != -1)
-				{
-					w[xi->index-1] += sign*(z-alpha_old)*yi*xi->value;
-					xi++;
-				}
+				sparse_operator::axpy(sign*(z-alpha_old)*yi, xi, w);
 			}
 		}
 
@@ -1402,12 +1428,12 @@ int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, dou
 	return iter;
 }
 
-// A coordinate descent algorithm for
+// A coordinate descent algorithm for
 // L1-regularized L2-loss support vector classification
 //
 //  min_w \sum |wj| + C \sum max(0, 1-yi w^T xi)^2,
 //
-// Given:
+// Given:
 // x, y, Cp, Cn
 // eps is the stopping tolerance
 //
@@ -1426,6 +1452,7 @@ static int solve_l1r_l2_svc(
 	int l = prob_col->l;
 	int w_size = prob_col->n;
 	int j, s, iter = 0;
+	//int max_iter = 1000;
 	int active_size = w_size;
 	int max_num_linesearch = 20;
 
@@ -1481,7 +1508,7 @@ static int solve_l1r_l2_svc(
 
 		for(j=0; j<active_size; j++)
 		{
-			int i = j+rand()%(active_size-j);
+			int i = j+rand_int(active_size-j);
 			swap(index[i], index[j]);
 		}
 
@@ -1558,11 +1585,7 @@ static int solve_l1r_l2_svc(
 				if(appxcond <= 0)
 				{
 					x = prob_col->x[j];
-					while(x->index != -1)
-					{
-						b[x->index-1] += d_diff*x->value;
-						x++;
-					}
+					sparse_operator::axpy(d_diff, x, b);
 					break;
 				}
 
@@ -1622,11 +1645,7 @@ static int solve_l1r_l2_svc(
 				{
 					if(w[i]==0) continue;
 					x = prob_col->x[i];
-					while(x->index != -1)
-					{
-						b[x->index-1] -= w[i]*x->value;
-						x++;
-					}
+					sparse_operator::axpy(-w[i], x, b);
 				}
 			}
 		}
@@ -1689,12 +1708,12 @@ static int solve_l1r_l2_svc(
 	return iter;
 }
 
-// A coordinate descent algorithm for
+// A coordinate descent algorithm for
 // L1-regularized logistic regression problems
 //
 //  min_w \sum |wj| + C \sum log(1+exp(-yi w^T xi)),
 //
-// Given:
+// Given:
 // x, y, Cp, Cn
 // eps is the stopping tolerance
 //
@@ -1713,6 +1732,7 @@ static int solve_l1r_lr(
 	int l = prob_col->l;
 	int w_size = prob_col->n;
 	int j, s, newton_iter=0, iter=0;
+	//int max_newton_iter = 100;
 	int max_iter = 1000;
 	int max_num_linesearch = 20;
 	int active_size;
@@ -1856,7 +1876,7 @@ static int solve_l1r_lr(
 
 			for(j=0; j<QP_active_size; j++)
 			{
-				int i = j+rand()%(QP_active_size-j);
+				int i = j+rand_int(QP_active_size-j);
 				swap(index[i], index[j]);
 			}
 
@@ -1915,12 +1935,7 @@ static int solve_l1r_lr(
 				wpd[j] += z;
 
 				x = prob_col->x[j];
-				while(x->index != -1)
-				{
-					int ind = x->index-1;
-					xTd[ind] += x->value*z;
-					x++;
-				}
+				sparse_operator::axpy(z, x, xTd);
 			}
 
 			iter++;
@@ -2012,11 +2027,7 @@ static int solve_l1r_lr(
 			{
 				if(w[i]==0) continue;
 				x = prob_col->x[i];
-				while(x->index != -1)
-				{
-					exp_wTx[x->index-1] += w[i]*x->value;
-					x++;
-				}
+				sparse_operator::axpy(w[i], x, exp_wTx);
 			}
 
 			for(int i=0; i<l; i++)
@@ -2239,8 +2250,13 @@ static void group_classes(const problem *prob, int *nr_class_ret, int **label_re
 
 static int train_one(const problem *prob, const parameter *param, double *w, double Cp, double Cn)
 {
-	double eps=param->eps;
-	int max_iter=param->max_iter;
+	//inner and outer tolerances for TRON
+	double eps = param->eps;
+    int max_iter=param->max_iter;
+	double eps_cg = 0.1;
+	if(param->init_sol != NULL)
+		eps_cg = 0.5;
+
 	int pos = 0;
 	int neg = 0;
 	int n_iter = -1;
@@ -2248,7 +2264,6 @@ static int train_one(const problem *prob, const parameter *param, double *w, dou
 		if(prob->y[i] > 0)
 			pos++;
 	neg = prob->l - pos;
-
 	double primal_solver_tol = eps*max(min(pos,neg), 1)/prob->l;
 
 	function *fun_obj=NULL;
@@ -2265,7 +2280,7 @@ static int train_one(const problem *prob, const parameter *param, double *w, dou
 					C[i] = Cn;
 			}
 			fun_obj=new l2r_lr_fun(prob, C);
-			TRON tron_obj(fun_obj, primal_solver_tol, max_iter);
+			TRON tron_obj(fun_obj, primal_solver_tol, eps_cg, max_iter);
 			tron_obj.set_print_string(liblinear_print_string);
 			n_iter=tron_obj.tron(w);
 			delete fun_obj;
@@ -2283,7 +2298,7 @@ static int train_one(const problem *prob, const parameter *param, double *w, dou
 					C[i] = Cn;
 			}
 			fun_obj=new l2r_l2_svc_fun(prob, C);
-			TRON tron_obj(fun_obj, primal_solver_tol, max_iter);
+			TRON tron_obj(fun_obj, primal_solver_tol, eps_cg, max_iter);
 			tron_obj.set_print_string(liblinear_print_string);
 			n_iter=tron_obj.tron(w);
 			delete fun_obj;
@@ -2328,7 +2343,7 @@ static int train_one(const problem *prob, const parameter *param, double *w, dou
 				C[i] = param->C;
 
 			fun_obj=new l2r_l2_svr_fun(prob, C, param->p);
-			TRON tron_obj(fun_obj, param->eps, max_iter);
+			TRON tron_obj(fun_obj, param->eps, /*eps_cg=*/0.1, max_iter);
 			tron_obj.set_print_string(liblinear_print_string);
 			n_iter=tron_obj.tron(w);
 			delete fun_obj;
@@ -2349,6 +2364,36 @@ static int train_one(const problem *prob, const parameter *param, double *w, dou
 	return n_iter;
 }
 
+// Calculate the initial C for parameter selection
+static double calc_start_C(const problem *prob, const parameter *param)
+{
+	int i;
+	double xTx,max_xTx;
+	max_xTx = 0;
+	for(i=0; i<prob->l; i++)
+	{
+		xTx = 0;
+		feature_node *xi=prob->x[i];
+		while(xi->index != -1)
+		{
+			double val = xi->value;
+			xTx += val*val;
+			xi++;
+		}
+		if(xTx > max_xTx)
+			max_xTx = xTx;
+	}
+
+	double min_C = 1.0;
+	if(param->solver_type == L2R_LR)
+		min_C = 1.0 / (prob->l * max_xTx);
+	else if(param->solver_type == L2R_L2LOSS_SVC)
+		min_C = 1.0 / (2 * prob->l * max_xTx);
+
+	return pow( 2, floor(log(min_C) / log(2.0)) );
+}
+
+
 //
 // Interface functions
 //
@@ -2358,7 +2403,7 @@ model* train(const problem *prob, const parameter *param)
 	int l = prob->l;
 	int n = prob->n;
 	int w_size = prob->n;
-	int n_iter;
+	//int n_iter;
 	model *model_ = Malloc(model,1);
 
 	if(prob->bias>=0)
@@ -2368,13 +2413,17 @@ model* train(const problem *prob, const parameter *param)
 	model_->param = *param;
 	model_->bias = prob->bias;
 
+	omp_set_num_threads(param->nr_thread);
+
 	if(check_regression_model(model_))
 	{
 		model_->w = Malloc(double, w_size);
 		model_->n_iter = Malloc(int, 1);
+		for(i=0; i<w_size; i++)
+			model_->w[i] = 0;
 		model_->nr_class = 2;
 		model_->label = NULL;
-		model_->n_iter[0] =train_one(prob, param, &model_->w[0], 0, 0);
+		model_->n_iter[0] = train_one(prob, param, model_->w, 0, 0);
 	}
 	else
 	{
@@ -2442,36 +2491,92 @@ model* train(const problem *prob, const parameter *param)
 				int e0 = start[0]+count[0];
 				k=0;
 				for(; k<e0; k++)
-					sub_prob.y[k] = -1;
-				for(; k<sub_prob.l; k++)
 					sub_prob.y[k] = +1;
+				for(; k<sub_prob.l; k++)
+					sub_prob.y[k] = -1;
+
+				if(param->init_sol != NULL)
+					for(i=0;i<w_size;i++)
+						model_->w[i] = param->init_sol[i];
+				else
+					for(i=0;i<w_size;i++)
+						model_->w[i] = 0;
 
-				model_->n_iter[0]=train_one(&sub_prob, param, &model_->w[0], weighted_C[1], weighted_C[0]);
+				model_->n_iter[0]=train_one(&sub_prob, param, model_->w, weighted_C[0], weighted_C[1]);
 			}
 			else
 			{
-				model_->w=Malloc(double, w_size*nr_class);
-				double *w=Malloc(double, w_size);
-				model_->n_iter=Malloc(int, nr_class);
-				for(i=0;i<nr_class;i++)
-				{
-					int si = start[i];
-					int ei = si+count[i];
-
-					k=0;
-					for(; k<si; k++)
-						sub_prob.y[k] = -1;
-					for(; k<ei; k++)
-						sub_prob.y[k] = +1;
-					for(; k<sub_prob.l; k++)
-						sub_prob.y[k] = -1;
-
-					model_->n_iter[i]=train_one(&sub_prob, param, w, weighted_C[i], param->C);
-
-					for(int j=0;j<w_size;j++)
-						model_->w[j*nr_class+i] = w[j];
-				}
-				free(w);
+                #ifndef CV_OMP
+                    model_->w=Malloc(double, w_size*nr_class);
+                    double *w=Malloc(double, w_size);
+                    model_->n_iter=Malloc(int, nr_class);
+                    for(i=0;i<nr_class;i++)
+                    {
+                        int si = start[i];
+                        int ei = si+count[i];
+
+                        k=0;
+                        for(; k<si; k++)
+                            sub_prob.y[k] = -1;
+                        for(; k<ei; k++)
+                            sub_prob.y[k] = +1;
+                        for(; k<sub_prob.l; k++)
+                            sub_prob.y[k] = -1;
+
+                        if(param->init_sol != NULL)
+                            for(j=0;j<w_size;j++)
+                                w[j] = param->init_sol[j*nr_class+i];
+                        else
+                            for(j=0;j<w_size;j++)
+                                w[j] = 0;
+
+                        model_->n_iter[i]=train_one(&sub_prob, param, w, weighted_C[i], param->C);
+
+                        for(int j=0;j<w_size;j++)
+                            model_->w[j*nr_class+i] = w[j];
+                    }
+                    free(w);
+                #else
+                    // multi-core multi class
+                    model_->w=Malloc(double, w_size*nr_class);
+                    model_->n_iter=Malloc(int, nr_class);
+                    #pragma omp parallel for private(i, j, k)
+                    for(i=0;i<nr_class;i++)
+                    {
+                        problem sub_prob_omp;
+                        sub_prob_omp.l = l;
+                        sub_prob_omp.n = n;
+                        sub_prob_omp.x = x;
+                        sub_prob_omp.y = Malloc(double,l);
+
+                        int si = start[i];
+                        int ei = si+count[i];
+
+                        double *w=Malloc(double, w_size);
+
+                        k=0;
+                        for(; k<si; k++)
+                            sub_prob_omp.y[k] = -1;
+                        for(; k<ei; k++)
+                            sub_prob_omp.y[k] = +1;
+                        for(; k<sub_prob_omp.l; k++)
+                            sub_prob_omp.y[k] = -1;
+
+                        if(param->init_sol != NULL)
+                            for(j=0;j<w_size;j++)
+                                w[j] = param->init_sol[j*nr_class+i];
+                        else
+                            for(j=0;j<w_size;j++)
+                                w[j] = 0;
+
+                        model_->n_iter[i]=train_one(&sub_prob_omp, param, w, weighted_C[i], param->C);
+
+                        for(j=0;j<w_size;j++)
+                            model_->w[j*nr_class+i] = w[j];
+                        free(sub_prob_omp.y);
+                        free(w);
+                    }
+                #endif
 			}
 
 		}
@@ -2488,7 +2593,6 @@ model* train(const problem *prob, const parameter *param)
 	return model_;
 }
 
-#if 0
 void cross_validation(const problem *prob, const parameter *param, int nr_fold, double *target)
 {
 	int i;
@@ -2504,12 +2608,15 @@ void cross_validation(const problem *prob, const parameter *param, int nr_fold,
 	for(i=0;i<l;i++) perm[i]=i;
 	for(i=0;i<l;i++)
 	{
-		int j = i+rand()%(l-i);
+		int j = i+rand_int(l-i);
 		swap(perm[i],perm[j]);
 	}
 	for(i=0;i<=nr_fold;i++)
 		fold_start[i]=i*l/nr_fold;
 
+#ifdef CV_OMP
+#pragma omp parallel for private(i) schedule(dynamic)
+#endif
 	for(i=0;i<nr_fold;i++)
 	{
 		int begin = fold_start[i];
@@ -2547,6 +2654,162 @@ void cross_validation(const problem *prob, const parameter *param, int nr_fold,
 	free(perm);
 }
 
+void find_parameter_C(const problem *prob, const parameter *param, int nr_fold, double start_C, double max_C, double *best_C, double *best_rate)
+{
+	// variables for CV
+	int i;
+	int *fold_start;
+	int l = prob->l;
+	int *perm = Malloc(int, l);
+	double *target = Malloc(double, prob->l);
+	struct problem *subprob = Malloc(problem,nr_fold);
+
+	// variables for warm start
+	double ratio = 2;
+	double **prev_w = Malloc(double*, nr_fold);
+	for(i = 0; i < nr_fold; i++)
+		prev_w[i] = NULL;
+	int num_unchanged_w = 0;
+	struct parameter param1 = *param;
+	void (*default_print_string) (const char *) = liblinear_print_string;
+
+	if (nr_fold > l)
+	{
+		nr_fold = l;
+		fprintf(stderr,"WARNING: # folds > # data. Will use # folds = # data instead (i.e., leave-one-out cross validation)\n");
+	}
+	fold_start = Malloc(int,nr_fold+1);
+	for(i=0;i<l;i++) perm[i]=i;
+	for(i=0;i<l;i++)
+	{
+		int j = i+rand_int(l-i);
+		swap(perm[i],perm[j]);
+	}
+	for(i=0;i<=nr_fold;i++)
+		fold_start[i]=i*l/nr_fold;
+
+	for(i=0;i<nr_fold;i++)
+	{
+		int begin = fold_start[i];
+		int end = fold_start[i+1];
+		int j,k;
+
+		subprob[i].bias = prob->bias;
+		subprob[i].n = prob->n;
+		subprob[i].l = l-(end-begin);
+		subprob[i].x = Malloc(struct feature_node*,subprob[i].l);
+		subprob[i].y = Malloc(double,subprob[i].l);
+
+		k=0;
+		for(j=0;j<begin;j++)
+		{
+			subprob[i].x[k] = prob->x[perm[j]];
+			subprob[i].y[k] = prob->y[perm[j]];
+			++k;
+		}
+		for(j=end;j<l;j++)
+		{
+			subprob[i].x[k] = prob->x[perm[j]];
+			subprob[i].y[k] = prob->y[perm[j]];
+			++k;
+		}
+
+	}
+
+	*best_rate = 0;
+	if(start_C <= 0)
+		start_C = calc_start_C(prob,param);
+	param1.C = start_C;
+
+	while(param1.C <= max_C)
+	{
+		//Output disabled for running CV at a particular C
+		set_print_string_function(&print_null);
+
+#ifdef CV_OMP
+#pragma omp parallel for private(i) schedule(dynamic)
+#endif
+		for(i=0; i<nr_fold; i++)
+		{
+			int j;
+			int begin = fold_start[i];
+			int end = fold_start[i+1];
+
+			struct parameter param_t = param1;
+			param_t.init_sol = prev_w[i];
+			struct model *submodel = train(&subprob[i],&param_t);
+
+			int total_w_size;
+			if(submodel->nr_class == 2)
+				total_w_size = subprob[i].n;
+			else
+				total_w_size = subprob[i].n * submodel->nr_class;
+
+			if(prev_w[i] == NULL)
+			{
+				prev_w[i] = Malloc(double, total_w_size);
+				for(j=0; j<total_w_size; j++)
+					prev_w[i][j] = submodel->w[j];
+			}
+			else if(num_unchanged_w >= 0)
+			{
+				double norm_w_diff = 0;
+				for(j=0; j<total_w_size; j++)
+				{
+					norm_w_diff += (submodel->w[j] - prev_w[i][j])*(submodel->w[j] - prev_w[i][j]);
+					prev_w[i][j] = submodel->w[j];
+				}
+				norm_w_diff = sqrt(norm_w_diff);
+
+				if(norm_w_diff > 1e-15)
+					num_unchanged_w = -1;
+			}
+			else
+			{
+				for(j=0; j<total_w_size; j++)
+					prev_w[i][j] = submodel->w[j];
+			}
+
+			for(j=begin; j<end; j++)
+				target[perm[j]] = predict(submodel,prob->x[perm[j]]);
+
+			free_and_destroy_model(&submodel);
+		}
+		set_print_string_function(default_print_string);
+
+		int total_correct = 0;
+		for(i=0; i<prob->l; i++)
+			if(target[i] == prob->y[i])
+				++total_correct;
+		double current_rate = (double)total_correct/prob->l;
+		if(current_rate > *best_rate)
+		{
+			*best_C = param1.C;
+			*best_rate = current_rate;
+		}
+
+		info("log2c=%7.2f\trate=%g\n",log(param1.C)/log(2.0),100.0*current_rate);
+		num_unchanged_w++;
+		if(num_unchanged_w == 3)
+			break;
+		param1.C = param1.C*ratio;
+	}
+
+	if(param1.C > max_C && max_C > start_C)
+		info("warning: maximum C reached.\n");
+	free(fold_start);
+	free(perm);
+	free(target);
+	for(i=0; i<nr_fold; i++)
+	{
+		free(subprob[i].x);
+		free(subprob[i].y);
+		free(prev_w[i]);
+	}
+	free(prev_w);
+	free(subprob);
+}
+
 double predict_values(const struct model *model_, const struct feature_node *x, double *dec_values)
 {
 	int idx;
@@ -2659,7 +2922,11 @@ int save_model(const char *model_file_name, const struct model *model_)
 	FILE *fp = fopen(model_file_name,"w");
 	if(fp==NULL) return -1;
 
-	char *old_locale = strdup(setlocale(LC_ALL, NULL));
+	char *old_locale = setlocale(LC_ALL, NULL);
+	if (old_locale)
+	{
+		old_locale = strdup(old_locale);
+	}
 	setlocale(LC_ALL, "C");
 
 	int nr_w;
@@ -2699,6 +2966,30 @@ int save_model(const char *model_file_name, const struct model *model_)
 	else return 0;
 }
 
+//
+// FSCANF helps to handle fscanf failures.
+// Its do-while block avoids the ambiguity when
+// if (...)
+//    FSCANF();
+// is used
+//
+#define FSCANF(_stream, _format, _var)do\
+{\
+	if (fscanf(_stream, _format, _var) != 1)\
+	{\
+		fprintf(stderr, "ERROR: fscanf failed to read the model\n");\
+		EXIT_LOAD_MODEL()\
+	}\
+}while(0)
+// EXIT_LOAD_MODEL should NOT end with a semicolon.
+#define EXIT_LOAD_MODEL()\
+{\
+	setlocale(LC_ALL, old_locale);\
+	free(model_->label);\
+	free(model_);\
+	free(old_locale);\
+	return NULL;\
+}
 struct model *load_model(const char *model_file_name)
 {
 	FILE *fp = fopen(model_file_name,"r");
@@ -2714,16 +3005,20 @@ struct model *load_model(const char *model_file_name)
 
 	model_->label = NULL;
 
-	char *old_locale = strdup(setlocale(LC_ALL, NULL));
+	char *old_locale = setlocale(LC_ALL, NULL);
+	if (old_locale)
+	{
+		old_locale = strdup(old_locale);
+	}
 	setlocale(LC_ALL, "C");
 
 	char cmd[81];
 	while(1)
 	{
-		fscanf(fp,"%80s",cmd);
+		FSCANF(fp,"%80s",cmd);
 		if(strcmp(cmd,"solver_type")==0)
 		{
-			fscanf(fp,"%80s",cmd);
+			FSCANF(fp,"%80s",cmd);
 			int i;
 			for(i=0;solver_type_table[i];i++)
 			{
@@ -2736,27 +3031,22 @@ struct model *load_model(const char *model_file_name)
 			if(solver_type_table[i] == NULL)
 			{
 				fprintf(stderr,"unknown solver type.\n");
-
-				setlocale(LC_ALL, old_locale);
-				free(model_->label);
-				free(model_);
-				free(old_locale);
-				return NULL;
+				EXIT_LOAD_MODEL()
 			}
 		}
 		else if(strcmp(cmd,"nr_class")==0)
 		{
-			fscanf(fp,"%d",&nr_class);
+			FSCANF(fp,"%d",&nr_class);
 			model_->nr_class=nr_class;
 		}
 		else if(strcmp(cmd,"nr_feature")==0)
 		{
-			fscanf(fp,"%d",&nr_feature);
+			FSCANF(fp,"%d",&nr_feature);
 			model_->nr_feature=nr_feature;
 		}
 		else if(strcmp(cmd,"bias")==0)
 		{
-			fscanf(fp,"%lf",&bias);
+			FSCANF(fp,"%lf",&bias);
 			model_->bias=bias;
 		}
 		else if(strcmp(cmd,"w")==0)
@@ -2768,16 +3058,12 @@ struct model *load_model(const char *model_file_name)
 			int nr_class = model_->nr_class;
 			model_->label = Malloc(int,nr_class);
 			for(int i=0;i<nr_class;i++)
-				fscanf(fp,"%d",&model_->label[i]);
+				FSCANF(fp,"%d",&model_->label[i]);
 		}
 		else
 		{
 			fprintf(stderr,"unknown text in model file: [%s]\n",cmd);
-			setlocale(LC_ALL, old_locale);
-			free(model_->label);
-			free(model_);
-			free(old_locale);
-			return NULL;
+			EXIT_LOAD_MODEL()
 		}
 	}
 
@@ -2798,8 +3084,12 @@ struct model *load_model(const char *model_file_name)
 	{
 		int j;
 		for(j=0; j<nr_w; j++)
-			fscanf(fp, "%lf ", &model_->w[i*nr_w+j]);
-		fscanf(fp, "\n");
+			FSCANF(fp, "%lf ", &model_->w[i*nr_w+j]);
+		if (fscanf(fp, "\n") !=0)
+		{
+			fprintf(stderr, "ERROR: fscanf failed to read the model\n");
+			EXIT_LOAD_MODEL()
+		}
 	}
 
 	setlocale(LC_ALL, old_locale);
@@ -2809,7 +3099,6 @@ struct model *load_model(const char *model_file_name)
 
 	return model_;
 }
-#endif
 
 int get_nr_feature(const model *model_)
 {
@@ -2840,9 +3129,8 @@ void get_n_iter(const model *model_, int* n_iter)
             n_iter[i] = model_->n_iter[i];
 }
 
-#if 0
 // use inline here for better performance (around 20% faster than the non-inline one)
-static inline double get_w_value(const struct model *model_, int idx, int label_idx)
+static inline double get_w_value(const struct model *model_, int idx, int label_idx)
 {
 	int nr_class = model_->nr_class;
 	int solver_type = model_->param.solver_type;
@@ -2852,7 +3140,7 @@ static inline double get_w_value(const struct model *model_, int idx, int label_
 		return 0;
 	if(check_regression_model(model_))
 		return w[idx];
-	else
+	else
 	{
 		if(label_idx < 0 || label_idx >= nr_class)
 			return 0;
@@ -2887,7 +3175,6 @@ double get_decfun_bias(const struct model *model_, int label_idx)
 	else
 		return bias*get_w_value(model_, bias_idx, label_idx);
 }
-#endif
 
 void free_model_content(struct model *model_ptr)
 {
@@ -2913,6 +3200,8 @@ void destroy_param(parameter* param)
 		free(param->weight_label);
 	if(param->weight != NULL)
 		free(param->weight);
+	if(param->init_sol != NULL)
+		free(param->init_sol);
 }
 
 const char *check_parameter(const problem *prob, const parameter *param)
@@ -2939,17 +3228,19 @@ const char *check_parameter(const problem *prob, const parameter *param)
 		&& param->solver_type != L2R_L1LOSS_SVR_DUAL)
 		return "unknown solver type";
 
+	if(param->init_sol != NULL
+		&& param->solver_type != L2R_LR && param->solver_type != L2R_L2LOSS_SVC)
+		return "Initial-solution specification supported only for solver L2R_LR and L2R_L2LOSS_SVC";
+
 	return NULL;
 }
 
-#if 0
 int check_probability_model(const struct model *model_)
 {
 	return (model_->param.solver_type==L2R_LR ||
 			model_->param.solver_type==L2R_LR_DUAL ||
 			model_->param.solver_type==L1R_LR);
 }
-#endif
 
 int check_regression_model(const struct model *model_)
 {
diff --git a/sklearn/svm/src/liblinear/linear.h b/sklearn/svm/src/liblinear/linear.h
index d3ef546..506728c 100644
--- a/sklearn/svm/src/liblinear/linear.h
+++ b/sklearn/svm/src/liblinear/linear.h
@@ -28,11 +28,13 @@ struct parameter
 	/* these are for training only */
 	double eps;	        /* stopping criteria */
 	double C;
+	int nr_thread;
 	int nr_weight;
 	int *weight_label;
 	double* weight;
 	int max_iter;
 	double p;
+	double *init_sol;
 };
 
 struct model
@@ -48,6 +50,7 @@ struct model
 
 struct model* train(const struct problem *prob, const struct parameter *param);
 void cross_validation(const struct problem *prob, const struct parameter *param, int nr_fold, double *target);
+void find_parameter_C(const struct problem *prob, const struct parameter *param, int nr_fold, double start_C, double max_C, double *best_C, double *best_rate);
 
 double predict_values(const struct model *model_, const struct feature_node *x, double* dec_values);
 double predict(const struct model *model_, const struct feature_node *x);
diff --git a/sklearn/svm/src/liblinear/tron.cpp b/sklearn/svm/src/liblinear/tron.cpp
index 1bf3204..0967f7b 100644
--- a/sklearn/svm/src/liblinear/tron.cpp
+++ b/sklearn/svm/src/liblinear/tron.cpp
@@ -12,9 +12,18 @@ template <class T> static inline T min(T x,T y) { return (x<y)?x:y; }
 template <class T> static inline T max(T x,T y) { return (x>y)?x:y; }
 #endif
 
+#ifdef __cplusplus
 extern "C" {
-#include <cblas.h>
+#endif
+
+extern double dnrm2_(int *, double *, int *);
+extern double ddot_(int *, double *, int *, double *, int *);
+extern int daxpy_(int *, double *, double *, int *, double *, int *);
+extern int dscal_(int *, double *, double *, int *);
+
+#ifdef __cplusplus
 }
+#endif
 
 static void default_print(const char *buf)
 {
@@ -32,10 +41,11 @@ void TRON::info(const char *fmt,...)
 	(*tron_print_string)(buf);
 }
 
-TRON::TRON(const function *fun_obj, double eps, int max_iter)
+TRON::TRON(const function *fun_obj, double eps, double eps_cg, int max_iter)
 {
 	this->fun_obj=const_cast<function *>(fun_obj);
 	this->eps=eps;
+	this->eps_cg=eps_cg;
 	this->max_iter=max_iter;
 	tron_print_string = default_print;
 }
@@ -54,44 +64,49 @@ int TRON::tron(double *w)
 
 	int n = fun_obj->get_nr_variable();
 	int i, cg_iter;
-	double delta, snorm;
+	double delta, snorm, one=1.0;
 	double alpha, f, fnew, prered, actred, gs;
 	int search = 1, iter = 1, inc = 1;
 	double *s = new double[n];
 	double *r = new double[n];
-	double *w_new = new double[n];
 	double *g = new double[n];
 
+	// calculate gradient norm at w=0 for stopping condition.
+	double *w0 = new double[n];
 	for (i=0; i<n; i++)
-		w[i] = 0;
+		w0[i] = 0;
+	fun_obj->fun(w0);
+	fun_obj->grad(w0, g);
+	double gnorm0 = dnrm2_(&n, g, &inc);
+	delete [] w0;
 
 	f = fun_obj->fun(w);
 	fun_obj->grad(w, g);
-	delta = cblas_dnrm2(n, g, inc);
-	double gnorm1 = delta;
-	double gnorm = gnorm1;
+	delta = dnrm2_(&n, g, &inc);
+	double gnorm = delta;
 
-	if (gnorm <= eps*gnorm1)
+	if (gnorm <= eps*gnorm0)
 		search = 0;
 
 	iter = 1;
 
+	double *w_new = new double[n];
 	while (iter <= max_iter && search)
 	{
 		cg_iter = trcg(delta, g, s, r);
 
 		memcpy(w_new, w, sizeof(double)*n);
-		cblas_daxpy(n, 1.0, s, inc, w_new, inc);
+		daxpy_(&n, &one, s, &inc, w_new, &inc);
 
-		gs = cblas_ddot(n, g, inc, s, inc);
-		prered = -0.5*(gs - cblas_ddot(n, s, inc, r, inc));
+		gs = ddot_(&n, g, &inc, s, &inc);
+		prered = -0.5*(gs-ddot_(&n, s, &inc, r, &inc));
 		fnew = fun_obj->fun(w_new);
 
 		// Compute the actual reduction.
 		actred = f - fnew;
 
 		// On the first iteration, adjust the initial step bound.
-		snorm = cblas_dnrm2(n, s, inc);
+		snorm = dnrm2_(&n, s, &inc);
 		if (iter == 1)
 			delta = min(delta, snorm);
 
@@ -120,8 +135,8 @@ int TRON::tron(double *w)
 			f = fnew;
 			fun_obj->grad(w, g);
 
-			gnorm = cblas_dnrm2(n, g, inc);
-			if (gnorm <= eps*gnorm1)
+			gnorm = dnrm2_(&n, g, &inc);
+			if (gnorm <= eps*gnorm0)
 				break;
 		}
 		if (f < -1.0e+32)
@@ -153,6 +168,7 @@ int TRON::trcg(double delta, double *g, double *s, double *r)
 {
 	int i, inc = 1;
 	int n = fun_obj->get_nr_variable();
+	double one = 1;
 	double *d = new double[n];
 	double *Hd = new double[n];
 	double rTr, rnewTrnew, alpha, beta, cgtol;
@@ -163,45 +179,45 @@ int TRON::trcg(double delta, double *g, double *s, double *r)
 		r[i] = -g[i];
 		d[i] = r[i];
 	}
-	cgtol = 0.1 * cblas_dnrm2(n, g, inc);
+	cgtol = eps_cg*dnrm2_(&n, g, &inc);
 
 	int cg_iter = 0;
-	rTr = cblas_ddot(n, r, inc, r, inc);
+	rTr = ddot_(&n, r, &inc, r, &inc);
 	while (1)
 	{
-		if (cblas_dnrm2(n, r, inc) <= cgtol)
+		if (dnrm2_(&n, r, &inc) <= cgtol)
 			break;
 		cg_iter++;
 		fun_obj->Hv(d, Hd);
 
-		alpha = rTr / cblas_ddot(n, d, inc, Hd, inc);
-		cblas_daxpy(n, alpha, d, inc, s, inc);
-		if (cblas_dnrm2(n, s, inc) > delta)
+		alpha = rTr/ddot_(&n, d, &inc, Hd, &inc);
+		daxpy_(&n, &alpha, d, &inc, s, &inc);
+		if (dnrm2_(&n, s, &inc) > delta)
 		{
 			info("cg reaches trust region boundary\n");
 			alpha = -alpha;
-			cblas_daxpy(n, alpha, d, inc, s, inc);
+			daxpy_(&n, &alpha, d, &inc, s, &inc);
 
-			double std = cblas_ddot(n, s, inc, d, inc);
-			double sts = cblas_ddot(n, s, inc, s, inc);
-			double dtd = cblas_ddot(n, d, inc, d, inc);
+			double std = ddot_(&n, s, &inc, d, &inc);
+			double sts = ddot_(&n, s, &inc, s, &inc);
+			double dtd = ddot_(&n, d, &inc, d, &inc);
 			double dsq = delta*delta;
 			double rad = sqrt(std*std + dtd*(dsq-sts));
 			if (std >= 0)
 				alpha = (dsq - sts)/(std + rad);
 			else
 				alpha = (rad - std)/dtd;
-			cblas_daxpy(n, alpha, d, inc, s, inc);
+			daxpy_(&n, &alpha, d, &inc, s, &inc);
 			alpha = -alpha;
-			cblas_daxpy(n, alpha, Hd, inc, r, inc);
+			daxpy_(&n, &alpha, Hd, &inc, r, &inc);
 			break;
 		}
 		alpha = -alpha;
-		cblas_daxpy(n, alpha, Hd, inc, r, inc);
-		rnewTrnew = cblas_ddot(n, r, inc, r, inc);
+		daxpy_(&n, &alpha, Hd, &inc, r, &inc);
+		rnewTrnew = ddot_(&n, r, &inc, r, &inc);
 		beta = rnewTrnew/rTr;
-		cblas_dscal(n, beta, d, inc);
-		cblas_daxpy(n, 1.0, r, inc, d, inc);
+		dscal_(&n, &beta, d, &inc);
+		daxpy_(&n, &one, r, &inc, d, &inc);
 		rTr = rnewTrnew;
 	}
 
diff --git a/sklearn/svm/src/liblinear/tron.h b/sklearn/svm/src/liblinear/tron.h
index 3349b83..5689a15 100644
--- a/sklearn/svm/src/liblinear/tron.h
+++ b/sklearn/svm/src/liblinear/tron.h
@@ -15,7 +15,7 @@ public:
 class TRON
 {
 public:
-	TRON(const function *fun_obj, double eps = 0.1, int max_iter = 1000);
+	TRON(const function *fun_obj, double eps = 0.1, double eps_cg = 0.1, int max_iter = 1000);
 	~TRON();
 
 	int tron(double *w);
@@ -26,6 +26,7 @@ private:
 	double norm_inf(int n, double *x);
 
 	double eps;
+	double eps_cg;
 	int max_iter;
 	function *fun_obj;
 	void info(const char *fmt,...);
diff --git a/sklearn/svm/src/libsvm/svm.cpp b/sklearn/svm/src/libsvm/svm.cpp
index f6d6882..db3f0ce 100644
--- a/sklearn/svm/src/libsvm/svm.cpp
+++ b/sklearn/svm/src/libsvm/svm.cpp
@@ -1417,6 +1417,7 @@ public:
 		int start, j;
 		if((start = cache->get_data(i,&data,len)) < len)
 		{
+            #pragma omp parallel for private(j) schedule(guided)
 			for(j=start;j<len;j++)
 				data[j] = (Qfloat)(y[i]*y[j]*(this->*kernel_function)(i,j));
 		}
@@ -1466,6 +1467,7 @@ public:
 		int start, j;
 		if((start = cache->get_data(i,&data,len)) < len)
 		{
+            #pragma omp parallel for private(j) schedule(guided)
 			for(j=start;j<len;j++)
 				data[j] = (Qfloat)(this->*kernel_function)(i,j);
 		}
@@ -2795,6 +2797,7 @@ double PREFIX(predict_values)(const PREFIX(model) *model, const PREFIX(node) *x,
 		double *sv_coef = model->sv_coef[0];
 		double sum = 0;
 		
+        #pragma omp parallel for private(i) reduction(+:sum) schedule(guided)
 		for(i=0;i<model->l;i++)
 #ifdef _DENSE_REP
                     sum += sv_coef[i] * NAMESPACE::Kernel::k_function(x,model->SV+i,model->param);
@@ -2815,6 +2818,7 @@ double PREFIX(predict_values)(const PREFIX(model) *model, const PREFIX(node) *x,
 		int l = model->l;
 		
 		double *kvalue = Malloc(double,l);
+        #pragma omp parallel for private(i) schedule(guided)
 		for(i=0;i<l;i++)
 #ifdef _DENSE_REP
                     kvalue[i] = NAMESPACE::Kernel::k_function(x,model->SV+i,model->param);
